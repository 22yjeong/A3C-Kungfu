{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/22yjeong/A3C-Kungfu/blob/main/Kung_fu_A3C_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIo6Zkp7U1Hq"
      },
      "source": [
        "# A3C for Kung Fu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqN2IEX1VKzi"
      },
      "source": [
        "### Installing Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbnq3XpoKa_7"
      },
      "outputs": [],
      "source": [
        "#install the game enviroment from the OpenAI gymnasium\n",
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrsNHNQqVZLK"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_25-9_9qnu"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributions as distributions\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "from gymnasium import ObservationWrapper\n",
        "from gymnasium.spaces import Box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyNc8cxbZCYP"
      },
      "source": [
        "### Creating the architecture of the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  # Intializes multiple layers of Convolutionl Nueral Networks\n",
        "  def __init__(self, action_size):\n",
        "    super(Network, self).__init__()\n",
        "    self.conv1 = torch.nn.Conv2d(in_channels = 4, out_channels = 32, kernel_size = (3,3), stride = 2)\n",
        "    self.conv2 = torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = (3,3), stride = 2)\n",
        "    self.conv3 = torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = (3,3), stride = 2)\n",
        "    self.flatten = torch.nn.Flatten()\n",
        "    self.fc1 = torch.nn.Linear(512,128)\n",
        "    self.fc2a = torch.nn.Linear(128,action_size)\n",
        "    self.fc2s = torch.nn.Linear(128,1)\n",
        "\n",
        "  # Connects the CNN Layers to the Nueral Network\n",
        "  def forward(self, state):\n",
        "    x = self.conv1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    action_values = self.fc2a(x)\n",
        "    state_value = self.fc2s(x)[0]\n",
        "    return action_values, state_value\n",
        "\n"
      ],
      "metadata": {
        "id": "CbIqTkskWHXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C2ydyKLZgaK"
      },
      "source": [
        "### Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF756uIhRVcK"
      },
      "outputs": [],
      "source": [
        "class PreprocessAtari(ObservationWrapper):\n",
        "  # initilaizes the Atari Class\n",
        "  def __init__(self, env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4):\n",
        "    super(PreprocessAtari, self).__init__(env)\n",
        "    self.img_size = (height, width)\n",
        "    self.crop = crop\n",
        "    self.dim_order = dim_order\n",
        "    self.color = color\n",
        "    self.frame_stack = n_frames\n",
        "    n_channels = 3 * n_frames if color else n_frames\n",
        "    obs_shape = {'tensorflow': (height, width, n_channels), 'pytorch': (n_channels, height, width)}[dim_order]\n",
        "    self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "    self.frames = np.zeros(obs_shape, dtype = np.float32)\n",
        "  # resets the frames\n",
        "  def reset(self):\n",
        "    self.frames = np.zeros_like(self.frames)\n",
        "    obs, info = self.env.reset()\n",
        "    self.update_buffer(obs)\n",
        "    return self.frames, info\n",
        "  # modifes the dimentions of the frames to fit the kernals of the CNN\n",
        "  def observation(self, img):\n",
        "    img = self.crop(img)\n",
        "    img = cv2.resize(img, self.img_size)\n",
        "    if not self.color:\n",
        "      if len(img.shape) == 3 and img.shape[2] == 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img = img.astype('float32') / 255.\n",
        "    if self.color:\n",
        "      self.frames = np.roll(self.frames, shift = -3, axis = 0)\n",
        "    else:\n",
        "      self.frames = np.roll(self.frames, shift = -1, axis = 0)\n",
        "    if self.color:\n",
        "      self.frames[-3:] = img\n",
        "    else:\n",
        "      self.frames[-1] = img\n",
        "    return self.frames\n",
        "\n",
        "  def update_buffer(self, obs):\n",
        "    self.frames = self.observation(obs)\n",
        "# creates the Enviroment of Kung fu\n",
        "def make_env():\n",
        "  env = gym.make(\"KungFuMasterDeterministic-v0\", render_mode = 'rgb_array')\n",
        "  env = PreprocessAtari(env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4)\n",
        "  return env\n",
        "\n",
        "env = make_env()\n",
        "\n",
        "state_shape = env.observation_space.shape\n",
        "number_actions = env.action_space.n\n",
        "print(\"Observation shape:\", state_shape)\n",
        "print(\"Number actions:\", number_actions)\n",
        "print(\"Action names:\", env.env.env.get_action_meanings())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgRlooBmC1hr"
      },
      "source": [
        "### Initializing the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "discount_factor = 0.99\n",
        "number_enviroments = 10"
      ],
      "metadata": {
        "id": "JMDsw7l1aQ-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg_LmSs9IoTX"
      },
      "source": [
        "### Implementing the A3C class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  # initilizes the Agent\n",
        "  def __init__(self, action_size):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.action_size = action_size\n",
        "    self.network = Network(action_size).to(self.device)\n",
        "    self.optimizer = torch.optim.Adam(self.network.parameters(), lr = learning_rate)\n",
        "  # Creates the possible moves for the Agent\n",
        "  def act(self, state):\n",
        "    if state.ndim == 3:\n",
        "      state = [state]\n",
        "    state = torch.tensor(state, dtype = torch.float32, device = self.device)\n",
        "    action_values, _ = self.network.forward(state)\n",
        "    policy = F.softmax(action_values, dim = -1)\n",
        "    return np.array([np.random.choice(len(p), p = p) for p in policy.detach().cpu().numpy()])\n",
        "  # returns the next state and updates the reward\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    batch_size = state.shape[0]\n",
        "    state = torch.tensor(state, dtype = torch.float32, device = self.device)\n",
        "    next_state = torch.tensor(next_state, dtype = torch.float32, device = self.device)\n",
        "    reward = torch.tensor(reward, dtype = torch.float32, device = self.device)\n",
        "    done = torch.tensor(done, dtype = torch.bool, device = self.device).to(torch.float32)\n",
        "    action_values, state_value = self.network(state)\n",
        "    _, next_state_value = self.network(next_state)\n",
        "    target_state_value = reward + discount_factor * next_state_value * (1 - done)\n",
        "    advantage = target_state_value - state_value\n",
        "    probs = F.softmax(action_values, dim = -1)\n",
        "    logprobs = F.log_softmax(action_values, dim = -1)\n",
        "    enropy = -torch.sum(probs * logprobs, axis = -1)\n",
        "    batch_idx = np.arange(batch_size)\n",
        "    logp_actions = logprobs[batch_idx, action]\n",
        "    actor_loss = -(logp_actions * advantage.detach()).mean() - 0.001 * enropy.mean()\n",
        "    critic_loss = F.mse_loss(target_state_value.detach(), state_value)\n",
        "    total_loss = actor_loss + critic_loss\n",
        "    self.optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    self.optimizer.step()\n"
      ],
      "metadata": {
        "id": "x1KQRt3Xayoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RnRukHDKFJ0"
      },
      "source": [
        "### Initializing the A3C agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(number_actions)"
      ],
      "metadata": {
        "id": "hZzSgvnGg0HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB5SpmoKP0aK"
      },
      "source": [
        "Evaluating the A3C agent on a single episode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import total_ordering\n",
        "# creates the evalutor for the agents\n",
        "def evaluate(agent, env, n_episodes = 1):\n",
        "  episodes_rewards = []\n",
        "  for _ in range(n_episodes):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    while True:\n",
        "      action = agent.act(state)\n",
        "      state, reward, done, info, _ = env.step(action[0])\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    episodes_rewards.append(total_reward)\n",
        "  return episodes_rewards\n",
        "\n"
      ],
      "metadata": {
        "id": "mpIbigj4hrvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVSqiyjiQeMd"
      },
      "source": [
        "### Testing multiple agents on multiple environments at the same time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnvBatch:\n",
        "  # creates multiples agents while on multiple enviroments\n",
        "  def __init__(self, n_envs = 10):\n",
        "    self.envs = [make_env() for _ in range(n_envs)]\n",
        "\n",
        "  def reset(self):\n",
        "    _states = []\n",
        "    for env in self.envs:\n",
        "      _states.append(env.reset()[0])\n",
        "    return np.array(_states)\n",
        "\n",
        "  def step(self, actions):\n",
        "    next_states, rewards, dones, infos, _ =  map(np.array,zip(*[env.step(a) for env, a in zip(self.envs, actions)]))\n",
        "    for i in range(len(self.envs)):\n",
        "      if dones[i]:\n",
        "        next_states[i] = self.envs[i].reset()[0]\n",
        "    return next_states, rewards, dones, infos\n"
      ],
      "metadata": {
        "id": "pIX1lXomi_bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69WZWB4oRx1P"
      },
      "source": [
        "### Training the A3C agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "env_batch = EnvBatch(number_enviroments)\n",
        "batch_states = env_batch.reset()\n",
        "# displays the learning progress the tqdm has\n",
        "with tqdm.trange(0, 3001) as progess_bar:\n",
        "  for i in progess_bar:\n",
        "    batch_actions = agent.act(batch_states)\n",
        "    batch_next_states, batch_rewards, batch_dones, _ = env_batch.step(batch_actions)\n",
        "    batch_rewards *= 0.01\n",
        "    agent.step(batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones)\n",
        "    batch_states = batch_next_states\n",
        "    if i % 1000 == 0:\n",
        "      print(\"Average agent reward: \", np.mean(evaluate(agent, env, n_episodes = 10)))"
      ],
      "metadata": {
        "id": "DThOzJy5WUhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kG_YR9YdmUM"
      },
      "source": [
        "## Part 3 - Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGkTuO6DxZ6B"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "# Creates the video of the Agent and the Enviroment\n",
        "def show_video_of_model(agent, env):\n",
        "  state, _ = env.reset()\n",
        "  done = False\n",
        "  frames = []\n",
        "  while not done:\n",
        "    frame = env.render()\n",
        "    frames.append(frame)\n",
        "    action = agent.act(state)\n",
        "    state, reward, done, _, _ = env.step(action[0])\n",
        "  env.close()\n",
        "  imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, env)\n",
        "# Displays a video to show the Agent's gameplay\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}